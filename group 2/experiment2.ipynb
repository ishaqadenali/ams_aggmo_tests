{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experiment2.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"FTUNwltVQ_PA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!pip uninstall torch --yes\n","!pip3 uninstall torch --yes\n","!pip3 install torch torchvision"],"execution_count":0,"outputs":[]},{"metadata":{"id":"K9pmia9aREsK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import sys, os\n","try:\n","  sys.path.remove('/usr/local/lib/python3.6/dist-packages/torch/optim')\n","except ValueError:\n","  sys.path.append('/usr/local/lib/python3.6/dist-packages/torch/optim')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UKu2xvXVRHiY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#couldn't find a nicer way to put custom optimizers into pytorch module. Spent too long trying. Ugly method ftw\n","\n","#sys.path.append('/usr/local/lib/python3.6/dist-packages/torch/optim')\n","\n","import math\n","import torch\n","from optimizer import Optimizer\n","\n","\n","class AMSAggMo(Optimizer):\n","    \"\"\"Implements AMSgrad algorithm with Aggregated Momentum.\n","\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n","            algorithm from the paper `On the Convergence of Adam and Beyond`_\n","\n","    .. _Adam\\: A Method for Stochastic Optimization:\n","        https://arxiv.org/abs/1412.6980\n","    .. _On the Convergence of Adam and Beyond:\n","        https://openreview.net/forum?id=ryQu7f-RZ\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, beta1=[0, 0.9, 0.99], beta2=0.999, eps=1e-8,\n","                 weight_decay=0, amsgrad=False):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        for i, beta in enumerate(beta1):\n","            if not 0.0 <= beta < 1.0:\n","                raise ValueError(\"Invalid beta1 parameter at index {}: {}\".format(i, beta))\n","        if not 0.0 <= beta2 < 1.0:\n","            raise ValueError(\"Invalid beta2 parameter: {}\".format(beta2))\n","        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad)\n","        super(AMSAggMo, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AMSAggMo, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault('amsgrad', False)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","        for group in self.param_groups:\n","                            \n","            beta1 = group['beta1']\n","            beta2 = group['beta2']\n","            \n","            max_beta = max(beta1)\n","            \n","            #print(avg_mom)\n","            \n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = {}\n","                    for beta in beta1:\n","                        state['exp_avg'][beta] =  torch.zeros_like(p.data)\n","                        \n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","                    # Maintains max of all exp. moving avg. of sq. grad. values\n","                    state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n","                    \n","                exp_avg_sq = state['exp_avg_sq']\n","                max_exp_avg_sq = state['max_exp_avg_sq']\n","                \n","                state['step'] += 1\n","\n","                exp_avg = torch.zeros_like(p.data)\n","                \n","                # Decay the first and second moment running average coefficient\n","                bias_correction1 = 0\n","                for beta in beta1:\n","                    buf = state['exp_avg'][beta]   \n","                    buf.mul_(beta).add_(1 - beta, grad)\n","                    exp_avg += buf/len(beta1)\n","                    bias_correction1 += (1 - beta**state['step'])/len(beta1)\n","\n","                \n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                # Maintains the maximum of all 2nd moment running avg. till now\n","                torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                # Use the max. for normalizing running avg. of gradient\n","                denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","                #print('exp_avg')\n","                #print(exp_avg)\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","        return loss\n","from optimizer import Optimizer, required\n","\n","\n","class AggMo(Optimizer):\n","    r\"\"\"Implements Aggregated Momentum Gradient Descent,\n","    as proposed in `Aggregated Momentum: Stability Through Passive Damping`_.\n","    Args:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float): learning rate\n","        momentum (list, optional): damping vector (default: [0, 0.9, 0.99])\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","    Example:\n","        >>> optimizer = torch.optim.AggMo(model.parameters(), lr=0.1, momentum=[0,0.9,0.99,0.999])\n","        >>> optimizer.zero_grad()\n","        >>> loss_fn(model(input), target).backward()\n","        >>> optimizer.step()\n","    .. _Aggregated Momentum: Stability Through Passive Damping:\n","        https://arxiv.org/abs/1804.00325\n","    \"\"\"\n","\n","    def __init__(self, params, lr=required, momentum=[0.0, 0.9, 0.99], weight_decay=0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        for i, mom in enumerate(momentum):\n","            if not 0.0 <= mom:\n","                raise ValueError(\"Invalid momentum parameter at index {}: {}\".format(i, mom))\n","        if not 0.0 <= weight_decay:\n","            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n","        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n","        super(AggMo, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AggMo, self).__setstate__(state)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            weight_decay = group['weight_decay']\n","            momentum = group['momentum']\n","            total_mom = float(len(momentum))\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                d_p = p.grad.data\n","                if weight_decay != 0:\n","                    d_p.add_(weight_decay, p.data)\n","                param_state = self.state[p]\n","                if 'momentum_buffer' not in param_state:\n","                    param_state['momentum_buffer'] = {}\n","                    for beta in momentum:\n","                        param_state['momentum_buffer'][beta] = torch.zeros_like(p.data)\n","                for beta in momentum:\n","                    buf = param_state['momentum_buffer'][beta]\n","                    buf.mul_(beta).add_(d_p)\n","                    p.data.sub_(group['lr'] / total_mom, buf)\n","        return loss\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GrhFiGQARJXu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# functions to show an image\n","\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"lYHn3dNqRMCA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":55},"outputId":"b7eeac65-9efb-49ac-8f7f-57d8875dcabd","executionInfo":{"status":"ok","timestamp":1526832419658,"user_tz":240,"elapsed":2926,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["#load test and train data\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                          shuffle=True, num_workers=8)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=False, num_workers=8)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"metadata":{"id":"gaqE5T-NRNXM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"a02bea90-affe-4d19-a234-62da9cb2f2ff","executionInfo":{"status":"ok","timestamp":1526832420570,"user_tz":240,"elapsed":444,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["print(torch.cuda.is_available())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"metadata":{"id":"H-gLOUFYRjQs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#SGD with Nesterov, Adam, AMSGrad, AMSAggMo and AggMo\n","net, net1, net2, net3, net4 = Net(), Net(), Net(), Net(), Net()\n","\n","#setup GPU use\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer2 = optim.Adam(net2.parameters(), lr=0.0005, amsgrad = True, betas = (0.99,0.999))\n","\n","epochs = 50"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z34mmOkiTfTs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":979},"outputId":"7fc88c12-bf02-49c0-b77e-22c75b10eb2f","executionInfo":{"status":"ok","timestamp":1526838014946,"user_tz":240,"elapsed":5589968,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["net2 = net2.to(device)\n","amsg_loss = np.zeros([1,epochs])\n","total = time.time()\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    avg_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer2.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net2(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer2.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        avg_loss += loss.item()\n","        if i % 12000 == 11999:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 12000))\n","            running_loss = 0.0\n","    amsg_loss[0,epoch] = avg_loss/(i+1)\n","\n","print('Finished Training AmsGrad. Time:', time.time()-total,'s')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[1, 12000] loss: 1.603\n","[2, 12000] loss: 1.289\n","[3, 12000] loss: 1.176\n","[4, 12000] loss: 1.100\n","[5, 12000] loss: 1.043\n","[6, 12000] loss: 0.995\n","[7, 12000] loss: 0.959\n","[8, 12000] loss: 0.918\n","[9, 12000] loss: 0.890\n","[10, 12000] loss: 0.857\n","[11, 12000] loss: 0.834\n","[12, 12000] loss: 0.807\n","[13, 12000] loss: 0.786\n","[14, 12000] loss: 0.771\n","[15, 12000] loss: 0.746\n","[16, 12000] loss: 0.729\n","[17, 12000] loss: 0.715\n","[18, 12000] loss: 0.697\n","[19, 12000] loss: 0.676\n","[20, 12000] loss: 0.663\n","[21, 12000] loss: 0.649\n","[22, 12000] loss: 0.638\n","[23, 12000] loss: 0.625\n","[24, 12000] loss: 0.613\n","[25, 12000] loss: 0.598\n","[26, 12000] loss: 0.591\n","[27, 12000] loss: 0.574\n","[28, 12000] loss: 0.565\n","[29, 12000] loss: 0.556\n","[30, 12000] loss: 0.550\n","[31, 12000] loss: 0.536\n","[32, 12000] loss: 0.524\n","[33, 12000] loss: 0.518\n","[34, 12000] loss: 0.509\n","[35, 12000] loss: 0.505\n","[36, 12000] loss: 0.492\n","[37, 12000] loss: 0.481\n","[38, 12000] loss: 0.477\n","[39, 12000] loss: 0.471\n","[40, 12000] loss: 0.457\n","[41, 12000] loss: 0.451\n","[42, 12000] loss: 0.450\n","[43, 12000] loss: 0.437\n","[44, 12000] loss: 0.438\n","[45, 12000] loss: 0.432\n","[46, 12000] loss: 0.416\n","[47, 12000] loss: 0.421\n","[48, 12000] loss: 0.410\n","[49, 12000] loss: 0.403\n","[50, 12000] loss: 0.392\n","Finished Training AmsGrad. Time: 5584.338452100754 s\n"],"name":"stdout"}]},{"metadata":{"id":"oL5Az1L9TtUG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"01b428e8-1a19-4874-d088-e50138735be7","executionInfo":{"status":"ok","timestamp":1526838056186,"user_tz":240,"elapsed":10248,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net2(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of AmsGrad on the 10000 test images: %f %%' % (\n","    100 * correct / float(total)))\n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Accuracy of AmsGrad on the 10000 test images: 59.650000 %\n"],"name":"stdout"}]},{"metadata":{"id":"H9KBY1UDUg8y","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":215},"outputId":"eb2e7ae3-0823-40f0-b591-b92807d958f2","executionInfo":{"status":"error","timestamp":1526838062874,"user_tz":240,"elapsed":476,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["for i in range(epochs):\n","  print(amsg_loss[])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["1.5941495674800872\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-e9955c21df23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamsg_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"]}]},{"metadata":{"id":"b5S9fboXTfy2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["amsg_loss.shape = (epochs,1)\n","\n","\n","x = np.linspace(1,epochs, num=epochs)\n","x.shape = (epochs,1)\n","\n","plt.figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n","plt.plot(x,amsg_loss,'g',label=\"AMSGrad\")\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Cross Entropy Loss')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L0boA8EamrUW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}