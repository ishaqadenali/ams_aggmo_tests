{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experiment5.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"noCGdUMO0NZi","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":263},"outputId":"b740ff8f-b63d-463f-ca19-feeaf61c2387","executionInfo":{"status":"ok","timestamp":1526790744082,"user_tz":240,"elapsed":32758,"user":{"displayName":"Ishaq Adenali","photoUrl":"//lh6.googleusercontent.com/-EfSng_f2yVw/AAAAAAAAAAI/AAAAAAAAAns/eadRMusiDZw/s50-c-k-no/photo.jpg","userId":"111576807901249688262"}}},"cell_type":"code","source":["!pip uninstall torch --yes\n","!pip3 uninstall torch --yes\n","!pip3 install torch torchvision"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Uninstalling torch-0.4.0:\n","  Successfully uninstalled torch-0.4.0\n","\u001b[33mSkipping torch as it is not installed.\u001b[0m\n","Collecting torch\n","  Using cached https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl\n","tcmalloc: large alloc 1073750016 bytes == 0x5b442000 @  0x7f7ade54c1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.1.0)\n","Installing collected packages: torch\n","Successfully installed torch-0.4.0\n"],"name":"stdout"}]},{"metadata":{"id":"JHhj3XS10VwM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import sys, os\n","try:\n","  sys.path.remove('/usr/local/lib/python3.6/dist-packages/torch/optim')\n","except ValueError:\n","  sys.path.append('/usr/local/lib/python3.6/dist-packages/torch/optim')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UW2PG2i70dhu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#couldn't find a nicer way to put custom optimizers into pytorch module. Spent too long trying. Ugly method ftw\n","\n","#sys.path.append('/usr/local/lib/python3.6/dist-packages/torch/optim')\n","\n","import math\n","import torch\n","from optimizer import Optimizer\n","\n","\n","class AMSAggMo(Optimizer):\n","    \"\"\"Implements AMSgrad algorithm with Aggregated Momentum.\n","\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n","            algorithm from the paper `On the Convergence of Adam and Beyond`_\n","\n","    .. _Adam\\: A Method for Stochastic Optimization:\n","        https://arxiv.org/abs/1412.6980\n","    .. _On the Convergence of Adam and Beyond:\n","        https://openreview.net/forum?id=ryQu7f-RZ\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, beta1=[0, 0.9, 0.99], beta2=0.999, eps=1e-8,\n","                 weight_decay=0, amsgrad=False):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        for i, beta in enumerate(beta1):\n","            if not 0.0 <= beta < 1.0:\n","                raise ValueError(\"Invalid beta1 parameter at index {}: {}\".format(i, beta))\n","        if not 0.0 <= beta2 < 1.0:\n","            raise ValueError(\"Invalid beta2 parameter: {}\".format(beta2))\n","        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad)\n","        super(AMSAggMo, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AMSAggMo, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault('amsgrad', False)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","        for group in self.param_groups:\n","                            \n","            beta1 = group['beta1']\n","            beta2 = group['beta2']\n","            \n","            max_beta = max(beta1)\n","            \n","            #print(avg_mom)\n","            \n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = {}\n","                    for beta in beta1:\n","                        state['exp_avg'][beta] =  torch.zeros_like(p.data)\n","                        \n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","                    # Maintains max of all exp. moving avg. of sq. grad. values\n","                    state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n","                    \n","                exp_avg_sq = state['exp_avg_sq']\n","                max_exp_avg_sq = state['max_exp_avg_sq']\n","                \n","                state['step'] += 1\n","\n","                exp_avg = torch.zeros_like(p.data)\n","                \n","                # Decay the first and second moment running average coefficient\n","                bias_correction1 = 0\n","                for beta in beta1:\n","                    buf = state['exp_avg'][beta]   \n","                    buf.mul_(beta).add_(1 - beta, grad)\n","                    exp_avg += buf/len(beta1)\n","                    bias_correction1 += (1 - beta**state['step'])/len(beta1)\n","\n","                \n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                # Maintains the maximum of all 2nd moment running avg. till now\n","                torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                # Use the max. for normalizing running avg. of gradient\n","                denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","                #print('exp_avg')\n","                #print(exp_avg)\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","        return loss\n","from optimizer import Optimizer, required\n","\n","\n","class AggMo(Optimizer):\n","    r\"\"\"Implements Aggregated Momentum Gradient Descent,\n","    as proposed in `Aggregated Momentum: Stability Through Passive Damping`_.\n","    Args:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float): learning rate\n","        momentum (list, optional): damping vector (default: [0, 0.9, 0.99])\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","    Example:\n","        >>> optimizer = torch.optim.AggMo(model.parameters(), lr=0.1, momentum=[0,0.9,0.99,0.999])\n","        >>> optimizer.zero_grad()\n","        >>> loss_fn(model(input), target).backward()\n","        >>> optimizer.step()\n","    .. _Aggregated Momentum: Stability Through Passive Damping:\n","        https://arxiv.org/abs/1804.00325\n","    \"\"\"\n","\n","    def __init__(self, params, lr=required, momentum=[0.0, 0.9, 0.99], weight_decay=0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        for i, mom in enumerate(momentum):\n","            if not 0.0 <= mom:\n","                raise ValueError(\"Invalid momentum parameter at index {}: {}\".format(i, mom))\n","        if not 0.0 <= weight_decay:\n","            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n","        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n","        super(AggMo, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AggMo, self).__setstate__(state)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            weight_decay = group['weight_decay']\n","            momentum = group['momentum']\n","            total_mom = float(len(momentum))\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                d_p = p.grad.data\n","                if weight_decay != 0:\n","                    d_p.add_(weight_decay, p.data)\n","                param_state = self.state[p]\n","                if 'momentum_buffer' not in param_state:\n","                    param_state['momentum_buffer'] = {}\n","                    for beta in momentum:\n","                        param_state['momentum_buffer'][beta] = torch.zeros_like(p.data)\n","                for beta in momentum:\n","                    buf = param_state['momentum_buffer'][beta]\n","                    buf.mul_(beta).add_(d_p)\n","                    p.data.sub_(group['lr'] / total_mom, buf)\n","        return loss\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aSGQ6glm0fHE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# functions to show an image\n","\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bwv1ynJw0kEg","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":55},"outputId":"ab92b447-3913-43ee-be1f-6b8dd4ec4ed6","executionInfo":{"status":"ok","timestamp":1526790779866,"user_tz":240,"elapsed":2190,"user":{"displayName":"Ishaq Adenali","photoUrl":"//lh6.googleusercontent.com/-EfSng_f2yVw/AAAAAAAAAAI/AAAAAAAAAns/eadRMusiDZw/s50-c-k-no/photo.jpg","userId":"111576807901249688262"}}},"cell_type":"code","source":["#load test and train data\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                          shuffle=True, num_workers=8)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=False, num_workers=8)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"metadata":{"id":"GREvhqrC0mos","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"b8dc4f69-c81a-4323-fa76-6d84c2a1d1b1","executionInfo":{"status":"ok","timestamp":1526790785638,"user_tz":240,"elapsed":492,"user":{"displayName":"Ishaq Adenali","photoUrl":"//lh6.googleusercontent.com/-EfSng_f2yVw/AAAAAAAAAAI/AAAAAAAAAns/eadRMusiDZw/s50-c-k-no/photo.jpg","userId":"111576807901249688262"}}},"cell_type":"code","source":["print(torch.cuda.is_available())"],"execution_count":6,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"metadata":{"id":"M4ONoFyX0oc8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#SGD with Nesterov, Adam, AMSGrad, AMSAggMo and AggMo\n","net, net1, net2, net3, net4 = Net(), Net(), Net(), Net(), Net()\n","\n","#setup GPU use\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.999, nesterov = True)\n","optimizer2 = optim.Adam(net2.parameters(), lr=0.01, amsgrad = True, betas = (0.999,0.999))\n","optimizer3 = AMSAggMo(net3.parameters(), lr=0.01, beta1 = [0,0.9,0.99,0.999])\n","optimizer4 = AggMo(net4.parameters(), lr = 0.01, momentum = [0,0.9,0.99,0.999])\n","\n","epochs = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4-youFUC0rSe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":225},"outputId":"fe481bfa-9688-4755-d8e0-b39ea74c2513","executionInfo":{"status":"ok","timestamp":1526792254276,"user_tz":240,"elapsed":1127304,"user":{"displayName":"Ishaq Adenali","photoUrl":"//lh6.googleusercontent.com/-EfSng_f2yVw/AAAAAAAAAAI/AAAAAAAAAns/eadRMusiDZw/s50-c-k-no/photo.jpg","userId":"111576807901249688262"}}},"cell_type":"code","source":["net2 = net2.to(device)\n","amsg_loss = np.zeros([1,epochs])\n","total = time.time()\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    avg_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer2.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net2(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer2.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        avg_loss += loss.item()\n","        if i % 12000 == 11999:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 12000))\n","            running_loss = 0.0\n","    amsg_loss[0,epoch] = avg_loss/(i+1)\n","\n","print('Finished Training AmsGrad. Time:', time.time()-total,'s')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[1, 12000] loss: 2.307\n","[2, 12000] loss: 2.305\n","[3, 12000] loss: 2.306\n","[4, 12000] loss: 2.306\n","[5, 12000] loss: 2.305\n","[6, 12000] loss: 2.306\n","[7, 12000] loss: 2.307\n","[8, 12000] loss: 2.307\n","[9, 12000] loss: 2.307\n","[10, 12000] loss: 2.306\n","Finished Training AmsGrad. Time: 1126.9763071537018 s\n"],"name":"stdout"}]},{"metadata":{"id":"aIQalYsf1768","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":225},"outputId":"b7230509-65b8-4e60-ea50-054a89b50e1d","executionInfo":{"status":"ok","timestamp":1526794131546,"user_tz":240,"elapsed":1877214,"user":{"displayName":"Ishaq Adenali","photoUrl":"//lh6.googleusercontent.com/-EfSng_f2yVw/AAAAAAAAAAI/AAAAAAAAAns/eadRMusiDZw/s50-c-k-no/photo.jpg","userId":"111576807901249688262"}}},"cell_type":"code","source":["net3 = net3.to(device)\n","amsaggmo_loss = np.zeros([1,epochs])\n","total = time.time()\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    avg_loss  = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer3.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net3(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer3.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        avg_loss += loss.item()\n","        if i % 12000 == 11999:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 12000))\n","            running_loss = 0.0\n","    amsaggmo_loss[0,epoch] = avg_loss / (i+1)\n","\n","\n","print('Finished Training AMSAggMo. Time:', time.time()-total,'s')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[1, 12000] loss: 2.306\n","[2, 12000] loss: 2.305\n","[3, 12000] loss: 2.305\n","[4, 12000] loss: 2.305\n","[5, 12000] loss: 2.305\n","[6, 12000] loss: 2.305\n","[7, 12000] loss: 2.305\n","[8, 12000] loss: 2.305\n","[9, 12000] loss: 2.305\n","[10, 12000] loss: 2.305\n","Finished Training AMSAggMo. Time: 1876.89745926857 s\n"],"name":"stdout"}]},{"metadata":{"id":"_m-VKptk1_FW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"b17d8d8d-6d9e-4bfb-f454-514e04ba56f1"},"cell_type":"code","source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net2(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of AmsGrad on the 10000 test images: %d %%' % (\n","    100 * correct / total))\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net3(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of AMSAggmo on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of AmsGrad on the 10000 test images: 10 %\n"],"name":"stdout"}]},{"metadata":{"id":"AHZwgmNh2Am4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#sgd_loss.shape = (epochs,1)\n","amsg_loss.shape = (epochs,1)\n","#aggmo_loss.shape = (epochs,1)\n","amsaggmo_loss.shape = (epochs,1)\n","\n","\n","x = np.linspace(1,epochs, num=epochs)\n","x.shape = (epochs,1)\n","\n","plt.figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n","#plt.plot(x,sgd_loss,'y',label =\"SGD-N\")\n","plt.plot(x,amsg_loss,'g',label=\"AMSGrad\")\n","#plt.plot(x,aggmo_loss,'k',label=\"AggMo\")\n","plt.plot(x,amsaggmo_loss,'r',label=\"AMSAggMo\")\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Cross Entropy Loss')\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KpCyzE2l2Dfk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}