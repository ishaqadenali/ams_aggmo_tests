{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"experiment12.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"FTUNwltVQ_PA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":395},"outputId":"d9ecb28d-71b6-4d66-a1e0-3476b571b2ac","executionInfo":{"status":"ok","timestamp":1526825857236,"user_tz":240,"elapsed":51470,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["!pip uninstall torch --yes\n","!pip3 uninstall torch --yes\n","!pip3 install torch torchvision"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[33mSkipping torch as it is not installed.\u001b[0m\n","\u001b[33mSkipping torch as it is not installed.\u001b[0m\n","Collecting torch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n","\u001b[K    88% |████████████████████████████▌   | 430.5MB 27.8MB/s eta 0:00:02"],"name":"stdout"},{"output_type":"stream","text":["\u001b[K    100% |████████████████████████████████| 484.0MB 25kB/s \n","tcmalloc: large alloc 1073750016 bytes == 0x5b194000 @  0x7f42877a01c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n","\u001b[?25hCollecting torchvision\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 14.3MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n","Collecting pillow>=4.1.1 (from torchvision)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/4b/8b54ab9d37b93998c81b364557dff9f61972c0f650efa0ceaf470b392740/Pillow-5.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K    100% |████████████████████████████████| 2.0MB 17.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.3)\n","Installing collected packages: torch, pillow, torchvision\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n","      Successfully uninstalled Pillow-4.0.0\n","Successfully installed pillow-5.1.0 torch-0.4.0 torchvision-0.2.1\n"],"name":"stdout"}]},{"metadata":{"id":"K9pmia9aREsK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import sys, os\n","try:\n","  sys.path.remove('/usr/local/lib/python3.6/dist-packages/torch/optim')\n","except ValueError:\n","  sys.path.append('/usr/local/lib/python3.6/dist-packages/torch/optim')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UKu2xvXVRHiY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#couldn't find a nicer way to put custom optimizers into pytorch module. Spent too long trying. Ugly method ftw\n","\n","#sys.path.append('/usr/local/lib/python3.6/dist-packages/torch/optim')\n","\n","import math\n","import torch\n","from optimizer import Optimizer\n","\n","\n","class AMSAggMo(Optimizer):\n","    \"\"\"Implements AMSgrad algorithm with Aggregated Momentum.\n","\n","    Arguments:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float, optional): learning rate (default: 1e-3)\n","        betas (Tuple[float, float], optional): coefficients used for computing\n","            running averages of gradient and its square (default: (0.9, 0.999))\n","        eps (float, optional): term added to the denominator to improve\n","            numerical stability (default: 1e-8)\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n","            algorithm from the paper `On the Convergence of Adam and Beyond`_\n","\n","    .. _Adam\\: A Method for Stochastic Optimization:\n","        https://arxiv.org/abs/1412.6980\n","    .. _On the Convergence of Adam and Beyond:\n","        https://openreview.net/forum?id=ryQu7f-RZ\n","    \"\"\"\n","\n","    def __init__(self, params, lr=1e-3, beta1=[0, 0.9, 0.99], beta2=0.999, eps=1e-8,\n","                 weight_decay=0, amsgrad=False):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n","        for i, beta in enumerate(beta1):\n","            if not 0.0 <= beta < 1.0:\n","                raise ValueError(\"Invalid beta1 parameter at index {}: {}\".format(i, beta))\n","        if not 0.0 <= beta2 < 1.0:\n","            raise ValueError(\"Invalid beta2 parameter: {}\".format(beta2))\n","        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps=eps,\n","                        weight_decay=weight_decay, amsgrad=amsgrad)\n","        super(AMSAggMo, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AMSAggMo, self).__setstate__(state)\n","        for group in self.param_groups:\n","            group.setdefault('amsgrad', False)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","        for group in self.param_groups:\n","                            \n","            beta1 = group['beta1']\n","            beta2 = group['beta2']\n","            \n","            max_beta = max(beta1)\n","            \n","            #print(avg_mom)\n","            \n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    # Exponential moving average of gradient values\n","                    state['exp_avg'] = {}\n","                    for beta in beta1:\n","                        state['exp_avg'][beta] =  torch.zeros_like(p.data)\n","                        \n","                    # Exponential moving average of squared gradient values\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","                    # Maintains max of all exp. moving avg. of sq. grad. values\n","                    state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n","                    \n","                exp_avg_sq = state['exp_avg_sq']\n","                max_exp_avg_sq = state['max_exp_avg_sq']\n","                \n","                state['step'] += 1\n","\n","                exp_avg = torch.zeros_like(p.data)\n","                \n","                # Decay the first and second moment running average coefficient\n","                bias_correction1 = 0\n","                for beta in beta1:\n","                    buf = state['exp_avg'][beta]   \n","                    buf.mul_(beta).add_(1 - beta, grad)\n","                    exp_avg += buf/len(beta1)\n","                    bias_correction1 += (1 - beta**state['step'])/len(beta1)\n","\n","                \n","                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n","                # Maintains the maximum of all 2nd moment running avg. till now\n","                torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n","                # Use the max. for normalizing running avg. of gradient\n","                denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n","\n","                bias_correction2 = 1 - beta2 ** state['step']\n","                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n","                #print('exp_avg')\n","                #print(exp_avg)\n","                p.data.addcdiv_(-step_size, exp_avg, denom)\n","\n","        return loss\n","from optimizer import Optimizer, required\n","\n","\n","class AggMo(Optimizer):\n","    r\"\"\"Implements Aggregated Momentum Gradient Descent,\n","    as proposed in `Aggregated Momentum: Stability Through Passive Damping`_.\n","    Args:\n","        params (iterable): iterable of parameters to optimize or dicts defining\n","            parameter groups\n","        lr (float): learning rate\n","        momentum (list, optional): damping vector (default: [0, 0.9, 0.99])\n","        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n","    Example:\n","        >>> optimizer = torch.optim.AggMo(model.parameters(), lr=0.1, momentum=[0,0.9,0.99,0.999])\n","        >>> optimizer.zero_grad()\n","        >>> loss_fn(model(input), target).backward()\n","        >>> optimizer.step()\n","    .. _Aggregated Momentum: Stability Through Passive Damping:\n","        https://arxiv.org/abs/1804.00325\n","    \"\"\"\n","\n","    def __init__(self, params, lr=required, momentum=[0.0, 0.9, 0.99], weight_decay=0):\n","        if not 0.0 <= lr:\n","            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n","        for i, mom in enumerate(momentum):\n","            if not 0.0 <= mom:\n","                raise ValueError(\"Invalid momentum parameter at index {}: {}\".format(i, mom))\n","        if not 0.0 <= weight_decay:\n","            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n","        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay)\n","        super(AggMo, self).__init__(params, defaults)\n","\n","    def __setstate__(self, state):\n","        super(AggMo, self).__setstate__(state)\n","\n","    def step(self, closure=None):\n","        \"\"\"Performs a single optimization step.\n","        Arguments:\n","            closure (callable, optional): A closure that reevaluates the model\n","                and returns the loss.\n","        \"\"\"\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            weight_decay = group['weight_decay']\n","            momentum = group['momentum']\n","            total_mom = float(len(momentum))\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                d_p = p.grad.data\n","                if weight_decay != 0:\n","                    d_p.add_(weight_decay, p.data)\n","                param_state = self.state[p]\n","                if 'momentum_buffer' not in param_state:\n","                    param_state['momentum_buffer'] = {}\n","                    for beta in momentum:\n","                        param_state['momentum_buffer'][beta] = torch.zeros_like(p.data)\n","                for beta in momentum:\n","                    buf = param_state['momentum_buffer'][beta]\n","                    buf.mul_(beta).add_(d_p)\n","                    p.data.sub_(group['lr'] / total_mom, buf)\n","        return loss\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GrhFiGQARJXu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","# functions to show an image\n","\n","\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"lYHn3dNqRMCA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":55},"outputId":"8c9ff34c-88d6-4414-8ec0-12c553f74484","executionInfo":{"status":"ok","timestamp":1526825897626,"user_tz":240,"elapsed":23464,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["#load test and train data\n","transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                          shuffle=True, num_workers=8)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=False, num_workers=8)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"metadata":{"id":"gaqE5T-NRNXM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":36},"outputId":"833d444e-98c8-4c31-e02e-7d9e13555051","executionInfo":{"status":"ok","timestamp":1526825898638,"user_tz":240,"elapsed":886,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["print(torch.cuda.is_available())"],"execution_count":6,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"metadata":{"id":"H-gLOUFYRjQs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["#SGD with Nesterov, Adam, AMSGrad, AMSAggMo and AggMo\n","net, net1, net2, net3, net4 = Net(), Net(), Net(), Net(), Net()\n","\n","#setup GPU use\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.0005, momentum=0.9, nesterov = True)\n","optimizer2 = optim.Adam(net2.parameters(), lr=0.0005, amsgrad = True, betas = (0.9,0.999))\n","optimizer3 = AMSAggMo(net3.parameters(), lr=0.0005, beta1 = [0,0.9])\n","optimizer4 = AggMo(net4.parameters(), lr = 0.0005, momentum = [0,0.9])\n","\n","epochs = 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dBnjOxUsS_V0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":225},"outputId":"88cb2230-14fa-4b12-e6f5-1bf920ac65bf","executionInfo":{"status":"ok","timestamp":1526826823406,"user_tz":240,"elapsed":919464,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["net = net.to(device)\n","sgd_loss = np.zeros([1,epochs])\n","total = time.time()\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    avg_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        avg_loss += loss.item()\n","        if i % 12000 == 11999:    # print every 12000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 12000))\n","            running_loss = 0.0\n","    sgd_loss[0,epoch] = avg_loss/(i+1)\n","\n","print('Finished TrainingNesterov SGD. Time:', time.time()-total,'s')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[1, 12000] loss: 1.853\n","[2, 12000] loss: 1.419\n","[3, 12000] loss: 1.259\n","[4, 12000] loss: 1.159\n","[5, 12000] loss: 1.088\n","[6, 12000] loss: 1.025\n","[7, 12000] loss: 0.973\n","[8, 12000] loss: 0.927\n","[9, 12000] loss: 0.883\n","[10, 12000] loss: 0.848\n","Finished TrainingNesterov SGD. Time: 914.6720795631409 s\n"],"name":"stdout"}]},{"metadata":{"id":"z34mmOkiTfTs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":225},"outputId":"3002ae9c-5817-4c2c-a7db-0ac3e3e16332","executionInfo":{"status":"ok","timestamp":1526827916886,"user_tz":240,"elapsed":1093386,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["net2 = net2.to(device)\n","amsg_loss = np.zeros([1,epochs])\n","total = time.time()\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    avg_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer2.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net2(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer2.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        avg_loss += loss.item()\n","        if i % 12000 == 11999:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 12000))\n","            running_loss = 0.0\n","    amsg_loss[0,epoch] = avg_loss/(i+1)\n","\n","print('Finished Training AmsGrad. Time:', time.time()-total,'s')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[1, 12000] loss: 1.548\n","[2, 12000] loss: 1.247\n","[3, 12000] loss: 1.124\n","[4, 12000] loss: 1.041\n","[5, 12000] loss: 0.975\n","[6, 12000] loss: 0.920\n","[7, 12000] loss: 0.874\n","[8, 12000] loss: 0.832\n","[9, 12000] loss: 0.793\n","[10, 12000] loss: 0.760\n","Finished Training AmsGrad. Time: 1092.8584735393524 s\n"],"name":"stdout"}]},{"metadata":{"id":"lu8cD8OcTmrK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":225},"outputId":"b775788f-0c40-49a3-ebde-182b86e6b8e3","executionInfo":{"status":"ok","timestamp":1526829364056,"user_tz":240,"elapsed":1447118,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["net3 = net3.to(device)\n","amsaggmo_loss = np.zeros([1,epochs])\n","total = time.time()\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    avg_loss  = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer3.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net3(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer3.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        avg_loss += loss.item()\n","        if i % 12000 == 11999:    # print every 2000 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 12000))\n","            running_loss = 0.0\n","    amsaggmo_loss[0,epoch] = avg_loss / (i+1)\n","\n","\n","print('Finished Training AMSAggMo. Time:', time.time()-total,'s')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[1, 12000] loss: 1.535\n","[2, 12000] loss: 1.251\n","[3, 12000] loss: 1.131\n","[4, 12000] loss: 1.046\n","[5, 12000] loss: 0.976\n","[6, 12000] loss: 0.920\n","[7, 12000] loss: 0.870\n","[8, 12000] loss: 0.828\n","[9, 12000] loss: 0.785\n","[10, 12000] loss: 0.751\n","Finished Training AMSAggMo. Time: 1446.610497713089 s\n"],"name":"stdout"}]},{"metadata":{"id":"IkEomTvXTqQY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":130},"outputId":"a7482efe-6f51-4e65-c934-10e68c8c6650","executionInfo":{"status":"ok","timestamp":1526830342290,"user_tz":240,"elapsed":978202,"user":{"displayName":"tempy tempants","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"110401299699843177305"}}},"cell_type":"code","source":["net4 = net4.to(device)\n","aggmo_loss = np.zeros([1,epochs])\n","total = time.time()\n","for epoch in range(epochs):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    avg_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer4.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net4(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer4.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        avg_loss += loss.item()\n","    if epoch % 2 == 1:    # print every 10 epochs\n","        print('end of epoch %d loss: %.3f' % (epoch + 1, running_loss / 15000))\n","    running_loss = 0.0\n","    aggmo_loss[0,epoch] = avg_loss/(i+1)\n","\n","print('Finished Training AggMo. Time:', time.time()-total,'s')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["end of epoch 2 loss: 1.299\n","end of epoch 4 loss: 1.075\n","end of epoch 6 loss: 0.948\n","end of epoch 8 loss: 0.852\n","end of epoch 10 loss: 0.782\n","Finished Training AggMo. Time: 977.7575056552887 s\n"],"name":"stdout"}]},{"metadata":{"id":"oL5Az1L9TtUG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":74},"outputId":"822ff071-5028-4afc-dd79-232a1016eaf2"},"cell_type":"code","source":["correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of SGD-N on the 10000 test images: %d %%' % (\n","    100 * correct / total))\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net2(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of AmsGrad on the 10000 test images: %d %%' % (\n","    100 * correct / total))\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net3(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of AMSAggmo on the 10000 test images: %d %%' % (\n","    100 * correct / total))\n","\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in testloader:\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = net4(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of AggMo on the 10000 test images: %d %%' % (\n","    100 * correct / total))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of SGD-N on the 10000 test images: 63 %\n","Accuracy of AmsGrad on the 10000 test images: 64 %\n","Accuracy of AMSAggmo on the 10000 test images: 63 %\n"],"name":"stdout"}]},{"metadata":{"id":"gbk9uQdGUY4e","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["sgd_loss.shape = (epochs,1)\n","amsg_loss.shape = (epochs,1)\n","aggmo_loss.shape = (epochs,1)\n","amsaggmo_loss.shape = (epochs,1)\n","\n","\n","x = np.linspace(1,epochs, num=epochs)\n","x.shape = (epochs,1)\n","\n","plt.figure(num=None, figsize=(16, 12), dpi=80, facecolor='w', edgecolor='k')\n","plt.plot(x,sgd_loss,'y',label =\"SGD-N\")\n","plt.plot(x,amsg_loss,'g',label=\"AMSGrad\")\n","plt.plot(x,aggmo_loss,'k',label=\"AggMo\")\n","plt.plot(x,amsaggmo_loss,'r',label=\"AMSAggMo\")\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Cross Entropy Loss')\n","plt.show()\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H9KBY1UDUg8y","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}